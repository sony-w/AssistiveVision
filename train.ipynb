{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from loader.dataset import VizwizDataset\n",
    "from loader.images import ImageS3\n",
    "from commons.utils import embedding_matrix, tensor_to_word_fn\n",
    "from models.resnext101 import TransformerAttention\n",
    "from eval.metrics import bleu, cider, rouge, spice, meteor, bleu_score_fn\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %pip install ipywidgets nltk\n",
    "# sudo yum install java-1.8.0-openjdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created! imgs = 23431, anns = 100575\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "train = VizwizDataset(dtype='train', ret_type='tensor', copy_img_to_mem=False, device=device, partial=100)\n",
    "vocabulary = train.getVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created! imgs = 7750, anns = 33145\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "val = VizwizDataset(dtype='val', ret_type='tensor', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.10s)\n",
      "creating index...\n",
      "index created! imgs = 7750, anns = 33145\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "val_eval = VizwizDataset(dtype='val', ret_type='corpus', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created! imgs = 8000, anns = 0\n",
      "no caption found... done!!\n"
     ]
    }
   ],
   "source": [
    "test = VizwizDataset(dtype='test', ret_type='tensor', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'resnext101_attention'\n",
    "GLOVE_DIR = 'annotations/glove'\n",
    "ENCODER_DIM = 2048\n",
    "EMBEDDING_DIM = 300\n",
    "ATTENTION_DIM = 256\n",
    "DECODER_DIM = 256\n",
    "\n",
    "BATCH_SIZE = 10 # 128\n",
    "LOG_INTERVAL = 25 * (256 // BATCH_SIZE)\n",
    "LR = 5e-4\n",
    "NUM_EPOCHS = 2\n",
    "SAVE_FREQ = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2585321f2b564fc18a9c41d98ac9e59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=378.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(378, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_mtx = embedding_matrix(embedding_dim=EMBEDDING_DIM, word2idx=vocabulary.word2idx, glove_dir=GLOVE_DIR)\n",
    "embedding_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, val_loader, model, loss_fn, optimizer, desc=''):\n",
    "    \n",
    "    # train model\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    t = tqdm(iter(train_loader), desc=f'{desc} ::: train')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, captions, lengths, fname = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scores, captions_sorted, decode_len, alphas, sort_idx = model(images, captions, lengths)\n",
    "        # exclude <start> and only includes after <start> to <end>\n",
    "        targets = captions_sorted[:, 1:]\n",
    "        # remove pads or timesteps that were not decoded\n",
    "        scores = pack_padded_sequence(scores, decode_len, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, decode_len, batch_first=True)[0]\n",
    "        \n",
    "        loss = loss_fn(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        t.set_postfix({\n",
    "            'loss': train_loss / (batch_idx + 1),\n",
    "            'acc': train_acc / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
    "            print(f'{desc}_train {batch_idx + 1}/{len(train_loader)} '\n",
    "                  f'train_loss: {train_loss / (batch_idx + 1):.4f} '\n",
    "                  f'train_acc: {train_acc / (batch_idx + 1):.4f}')\n",
    "            \n",
    "    train_loss_mean = train_loss / len(train_loader)\n",
    "\n",
    "    # validate model\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    t_val = tqdm(iter(val_loader), desc=f'{desc} ::: val')\n",
    "    for batch_idx, batch in enumerate(t_val):\n",
    "        images, captions, lengths, fname = batch\n",
    "        \n",
    "        scores, captions_sorted, decode_len, alphas, sort_idx = model(images, captions, lengths)\n",
    "        # exclude <start> and only includes after <start> to <end>\n",
    "        targets = captions_sorted[:, 1:]\n",
    "        # remove pads or timesteps that were not decoded\n",
    "        scores = pack_padded_sequence(scores, decode_len, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, decode_len, batch_first=True)[0]\n",
    "        \n",
    "        loss = loss_fn(scores, targets)\n",
    "        val_acc += (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        t_val.set_postfix({\n",
    "            'loss': val_loss / (batch_idx + 1),\n",
    "            'acc': val_acc / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
    "            print(f'{desc}_val {batch_idx + 1}/{len(val_loader)} '\n",
    "                  f'val_loss: {val_loss / (batch_idx + 1):.4f} '\n",
    "                  f'val_acc: {val_acc / (batch_idx + 1):.4f}')\n",
    "    \n",
    "    val_loss_mean = val_loss / len(val_loader)\n",
    "    \n",
    "    return train_loss_mean, val_loss_mean\n",
    "\n",
    "def detokenize(tokens):\n",
    "    return ''.join([' ' + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n",
    "\n",
    "def evaluate(data_loader, model, bleu_score_fn, tensor_to_word_fn, device=torch.device('cpu'), desc=''):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    pred_byfname = dict()\n",
    "    caps_byfname = defaultdict(list)\n",
    "    scores = dict()\n",
    "    \n",
    "    running_bleu = [0.0] * 5\n",
    "    \n",
    "    t = tqdm(iter(data_loader), desc=f'{desc}')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, captions, lengths, fname = batch\n",
    "        outputs = tensor_to_word_fn(model.sample(images, startseq_idx=vocabulary.word2idx['<start>']).cpu().numpy())\n",
    "        \n",
    "        for i in range(1, 5):\n",
    "            running_bleu[i] += bleu_score_fn(reference_corpus=captions, candidate_corpus=outputs, n=i)\n",
    "        t.set_postfix({\n",
    "            'bleu1': running_bleu[1] / (batch_idx + 1),\n",
    "            'bleu4': running_bleu[4] / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        for f, o, c in zip(fname, outputs, captions):\n",
    "            if not f in pred_byfname:\n",
    "                pred_byfname[f] = [detokenize(o)]\n",
    "            caps_byfname[f].append(detokenize(c))\n",
    "\n",
    "    # mean running_bleu score\n",
    "    for i in range(1, 5):\n",
    "        running_bleu[i] /= len(data_loader)\n",
    "    scores['avg_bleu'] = running_bleu\n",
    "\n",
    "    # calculate overall score\n",
    "    _bleu = bleu(caps_byfname, pred_byfname, verbose=0)\n",
    "    _cider = cider(caps_byfname, pred_byfname)\n",
    "    _rouge = rouge(caps_byfname, pred_byfname)\n",
    "    #_spice = spice(caps_byfname, pred_byfname)\n",
    "    #_meteor = meteor(caps_byfname, pred_byfname)\n",
    "    \n",
    "    scores['bleu'] = _bleu\n",
    "    scores['cider'] = _cider\n",
    "    scores['rouge'] = _rouge\n",
    "    #scores['spice'] = _spice\n",
    "    #scores['meteor'] = _meteor\n",
    "    \n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary.vocab)\n",
    "transformer = TransformerAttention(encoded_image_size=14, attention_dim=ATTENTION_DIM, embedding_dim=EMBEDDING_DIM, \n",
    "                                   decoder_dim=DECODER_DIM, vocab_size=vocab_size, encoder_dim=ENCODER_DIM, \n",
    "                                   embedding_matrix=embedding_mtx, train_embedding=True).to(device) \n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train.pad_value).to(device)\n",
    "corpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n",
    "tensor2word_fn = tensor_to_word_fn(idx2word=vocabulary.idx2word)\n",
    "\n",
    "params = transformer.parameters()\n",
    "optimizer = torch.optim.Adam(params=params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "eval_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train.transformations = train_transformations\n",
    "val.transformations = train_transformations\n",
    "val_eval.transformations = eval_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    "\n",
    "eval_collate_fn = lambda batch: (torch.stack([x[0] for x in batch]), [x[1] for x in batch], [x[2] for x in batch], [x[3] for x in batch])\n",
    "val_eval_loader = DataLoader(val_eval, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False, collate_fn=eval_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f17da38ef2a454090c57d7729309f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1 of 2 ::: train'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a610bdc46d04e4f8716c6f683155c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1 of 2 ::: val'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0741e55328944ec2b8ca75a4d9551608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Eval Score'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "===============================================================================================\n",
      "val_avg_bleu1: 0.1956 val_avg_bleu2: 0.0363 val_avg_bleu3: 0.0175 val_avg_bleu4: 0.0108 \n",
      "val_bleu1    : 0.3790 val_bleu2    : 0.1567 val_bleu3    : 0.0000 val_bleu4    : 0.0000 \n",
      "val_cider    : 0.0078\n",
      "val_rouge    : 0.2969\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7261afd74acc4b94ad698f5cf2c447cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2 of 2 ::: train'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b637f52bc7cd468bad723688a8781e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2 of 2 ::: val'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6409f7952de54e10b81bdd105b3e97ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Eval Score'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/2\n",
      "===============================================================================================\n",
      "val_avg_bleu1: 0.2076 val_avg_bleu2: 0.0397 val_avg_bleu3: 0.0195 val_avg_bleu4: 0.0124 \n",
      "val_bleu1    : 0.3917 val_bleu2    : 0.1834 val_bleu3    : 0.0000 val_bleu4    : 0.0000 \n",
      "val_cider    : 0.0148\n",
      "val_rouge    : 0.2871\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loss_min = 100\n",
    "val_loss_min = 100\n",
    "val_bleu4_max = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, val_loss = fit(train_loader, val_loader, model=transformer, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                              desc=f'Epoch {epoch+1} of {NUM_EPOCHS}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores =  evaluate(val_eval_loader, model=transformer, bleu_score_fn=corpus_bleu_score_fn, \n",
    "                           tensor_to_word_fn=tensor2word_fn, desc='Eval Score')\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "        print('=' * 95)\n",
    "        print(''.join([f'val_avg_bleu{i}: {scores[\"avg_bleu\"][i]:.4f} ' for i in range(1, 5)]))\n",
    "        print(''.join([f'val_bleu{i + 1}{\":\":>5} {scores[\"bleu\"][0][i]:.4f} ' for i in range(0, 4)]))\n",
    "        print(f'val_cider{\":\":>5} {scores[\"cider\"][0]:.4f}')\n",
    "        print(f'val_rouge{\":\":>5} {scores[\"rouge\"][0]:.4f}')\n",
    "        #print(f'val_spcie{\":\":>5} {scores[\"spice\"][0]:.4f}')\n",
    "        #print(f'val_meteor{\":\":>5} {scores[\"meteor\"][0]:.4f}')\n",
    "        print('-' * 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train.transformations = train_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eval_loader = DataLoader(val_eval, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False, collate_fn=eval_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95dc1aff1444a42bea008c16f07eb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='train'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg'] [['a', 'computer', 'screen', 'shows', 'a', 'repair', 'prompt', 'on', 'the', 'screen', '.'], ['a', 'computer', 'screen', 'with', 'a', 'repair', 'automatically', 'pop', 'up'], ['partial', 'computer', 'screen', 'showing', 'the', 'need', 'of', 'repairs'], ['part', 'of', 'a', 'computer', 'monitor', 'showing', 'a', 'computer', 'repair', 'message', '.'], ['the', 'top', 'of', 'a', 'laptop', 'with', 'a', 'blue', 'background', 'and', 'dark', 'blue', 'text', '.'], ['a', 'person', 'is', 'holding', 'a', 'bottle', 'that', 'has', 'medicine', 'for', 'the', 'night', 'time', '.'], ['a', 'bottle', 'of', 'medication', 'has', 'a', 'white', 'twist', 'top', '.'], ['night', 'time', 'medication', 'bottle', 'being', 'held', 'by', 'someone'], ['a', 'person', 'holding', 'a', 'small', 'black', 'bottle', 'of', 'night', 'time'], ['a', 'bottle', 'of', 'what', 'appears', 'to', 'be', 'cough', 'syrup', 'held', 'in', 'hand', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = tqdm(iter(val_eval_loader), desc=f'train')\n",
    "for batch_idx, batch in enumerate(t):\n",
    "    images, captions, lengths, fname = batch\n",
    "    if batch_idx == 0:\n",
    "        print(batch_idx, fname, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
