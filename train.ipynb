{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from loader.dataset import VizwizDataset\n",
    "from loader.images import ImageS3\n",
    "from commons.utils import embedding_matrix, tensor_to_word_fn\n",
    "from models.resnext101 import TransformerAttention\n",
    "from eval.metrics import bleu, cider, rouge, spice, meteor, bleu_score_fn\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %pip install ipywidgets nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.46s)\n",
      "creating index...\n",
      "index created! imgs = 23431, anns = 100575\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "train = VizwizDataset(dtype='train', ret_type='tensor', copy_img_to_mem=False, device=device, partial=100)\n",
    "vocabulary = train.getVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created! imgs = 7750, anns = 33145\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "val = VizwizDataset(dtype='val', ret_type='tensor', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created! imgs = 7750, anns = 33145\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "val_eval = VizwizDataset(dtype='val', ret_type='corpus', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created! imgs = 8000, anns = 0\n",
      "no caption found... done!!\n"
     ]
    }
   ],
   "source": [
    "test = VizwizDataset(dtype='test', ret_type='tensor', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'resnext101_attention'\n",
    "GLOVE_DIR = 'annotations/glove'\n",
    "ENCODER_DIM = 2048\n",
    "EMBEDDING_DIM = 300\n",
    "ATTENTION_DIM = 256\n",
    "DECODER_DIM = 256\n",
    "\n",
    "BATCH_SIZE = 10 # 128\n",
    "LOG_INTERVAL = 25 * (256 // BATCH_SIZE)\n",
    "LR = 5e-4\n",
    "NUM_EPOCHS = 2\n",
    "SAVE_FREQ = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9716e236801047c79111f5d03acd2b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=378.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(378, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_mtx = embedding_matrix(embedding_dim=EMBEDDING_DIM, word2idx=vocabulary.word2idx, glove_dir=GLOVE_DIR)\n",
    "embedding_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, val_loader, model, loss_fn, optimizer, desc=''):\n",
    "    \n",
    "    # train model\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    t = tqdm(iter(train_loader), desc=f'{desc} ::: train')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, captions, lengths, fname = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scores, captions_sorted, decode_len, alphas, sort_idx = model(images, captions, lengths)\n",
    "        # exclude <start> and only includes after <start> to <end>\n",
    "        targets = captions_sorted[:, 1:]\n",
    "        # remove pads or timesteps that were not decoded\n",
    "        scores = pack_padded_sequence(scores, decode_len, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, decode_len, batch_first=True)[0]\n",
    "        \n",
    "        loss = loss_fn(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        t.set_postfix({\n",
    "            'loss': train_loss / (batch_idx + 1),\n",
    "            'acc': train_acc / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
    "            print(f'{desc}_train {batch_idx + 1}/{len(train_loader)} '\n",
    "                  f'train_loss: {train_loss / (batch_idx + 1):.4f} '\n",
    "                  f'train_acc: {train_acc / (batch_idx + 1):.4f}')\n",
    "            \n",
    "    train_loss_mean = train_loss / len(train_loader)\n",
    "\n",
    "    # validate model\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    t_val = tqdm(iter(val_loader), desc=f'{desc} ::: val')\n",
    "    for batch_idx, batch in enumerate(t_val):\n",
    "        images, captions, lengths, fname = batch\n",
    "        \n",
    "        scores, captions_sorted, decode_len, alphas, sort_idx = model(images, captions, lengths)\n",
    "        # exclude <start> and only includes after <start> to <end>\n",
    "        targets = captions_sorted[:, 1:]\n",
    "        # remove pads or timesteps that were not decoded\n",
    "        scores = pack_padded_sequence(scores, decode_len, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, decode_len, batch_first=True)[0]\n",
    "        \n",
    "        loss = loss_fn(scores, targets)\n",
    "        val_acc += (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        t_val.set_postfix({\n",
    "            'loss': val_loss / (batch_idx + 1),\n",
    "            'acc': val_acc / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
    "            print(f'{desc}_val {batch_idx + 1}/{len(val_loader)} '\n",
    "                  f'val_loss: {val_loss / (batch_idx + 1):.4f} '\n",
    "                  f'val_acc: {val_acc / (batch_idx + 1):.4f}')\n",
    "    \n",
    "    val_loss_mean = val_loss / len(val_loader)\n",
    "    \n",
    "    return train_loss_mean, val_loss_mean\n",
    "\n",
    "def detokenize(tokens):\n",
    "    return ''.join([' ' + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n",
    "\n",
    "def evaluate(data_loader, model, bleu_score_fn, tensor_to_word_fn, device=torch.device('cpu'), desc=''):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    pred_byfname = dict()\n",
    "    caps_byfname = defaultdict(list)\n",
    "    scores = dict()\n",
    "    \n",
    "    bleus, ciders, rouges = [], [], []\n",
    "    running_bleu = [0.0] * 5\n",
    "    \n",
    "    t = tqdm(iter(data_loader), desc=f'{desc}')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, captions, lengths, fname = batch\n",
    "        outputs = tensor_to_word_fn(model.sample(images, startseq_idx=vocabulary.word2idx['<start>']).cpu().numpy())\n",
    "        \n",
    "        for i in range(1, 5):\n",
    "            running_bleu[i] += bleu_score_fn(reference_corpus=captions, candidate_corpus=outputs, n=i)\n",
    "        t.set_postfix({\n",
    "            'bleu1': running_bleu[1] / (batch_idx + 1),\n",
    "            'bleu4': running_bleu[4] / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        for f, o, c in zip(fname, outputs, captions):\n",
    "            if not f in pred_byfname:\n",
    "                pred_byfname[f] = [detokenize(o)]\n",
    "            caps_byfname[f].append(detokenize(c))\n",
    "\n",
    "    # mean running_bleu score\n",
    "    for i in range(1, 5):\n",
    "        running_bleu[i] /= len(data_loader)\n",
    "    scores['running_bleu'] = running_bleu\n",
    "\n",
    "    # calculate overall score\n",
    "    _bleu = bleu(caps_byfname, pred_byfname)\n",
    "    _cider = cider(caps_byfname, pred_byfname)\n",
    "    _rouge = rouge(caps_byfname, pred_byfname)\n",
    "    \n",
    "    bleus.append(_bleu)\n",
    "    ciders.append(_cider)\n",
    "    rouges.append(_rouge)\n",
    "    \n",
    "    scores['bleu'] = bleus\n",
    "    scores['cider'] = ciders\n",
    "    scores['rouge'] = rouges\n",
    "    \n",
    "    print('bleu', scores['bleu'])\n",
    "    print('cider', scores['cider'])\n",
    "    print('rouge', scores['rouge'])\n",
    "    \n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary.vocab)\n",
    "transformer = TransformerAttention(encoded_image_size=14, attention_dim=ATTENTION_DIM, embedding_dim=EMBEDDING_DIM, \n",
    "                                   decoder_dim=DECODER_DIM, vocab_size=vocab_size, encoder_dim=ENCODER_DIM, \n",
    "                                   embedding_matrix=embedding_mtx, train_embedding=True).to(device) \n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train.pad_value).to(device)\n",
    "corpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n",
    "tensor2word_fn = tensor_to_word_fn(idx2word=vocabulary.idx2word)\n",
    "\n",
    "params = transformer.parameters()\n",
    "optimizer = torch.optim.Adam(params=params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "eval_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train.transformations = train_transformations\n",
    "val.transformations = train_transformations\n",
    "val_eval.transformations = eval_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    "\n",
    "eval_collate_fn = lambda batch: (torch.stack([x[0] for x in batch]), [x[1] for x in batch], [x[2] for x in batch], [x[3] for x in batch])\n",
    "val_eval_loader = DataLoader(val_eval, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False, collate_fn=eval_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b518ef0b78418da8976e472e64f8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1 of 2 ::: train'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d691633dcd942fc97c1c596a6b32a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1 of 2 ::: val'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17e31baee6143d99de3e4856877ac05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Eval Score'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'testlen': 91, 'reflen': 107, 'guess': [91, 79, 67, 55], 'correct': [35, 2, 0, 0]}\n",
      "ratio: 0.8504672897116778\n",
      "bleu [([0.3226020944606647, 0.08276663693869638, 4.4098665638249194e-07, 1.0693959787638436e-09], [[0.23884377011164867, 0.23884377011164867, 0.5515605639774822, 0.29999999994000015, 0.3715190997867868, 0.24767939985785795, 0.33093633838648934, 0.2388437701381868, 0.33093633838648934, 0.5367694949004131, 0.22222222217283966, 0.23884377011164867], [5.85045365006396e-09, 5.85045365006396e-09, 0.26369638633624565, 5.773502690709485e-09, 0.23168286400471463, 5.982025825945981e-09, 6.459215935396237e-09, 4.62518972042083e-09, 6.459215935396237e-09, 8.199289470316909e-09, 5.270462765739487e-09, 5.85045365006396e-09], [1.830282338978762e-11, 1.830282338978762e-11, 2.170651479684559e-06, 1.60914897400152e-11, 2.103416377214603e-06, 1.837503456341076e-11, 1.8307983629515338e-11, 1.2985701392916413e-11, 1.8307983629515338e-11, 2.1618858818179448e-11, 1.5831904189457608e-11, 1.830282338978762e-11], [1.1000617045598687e-12, 1.1000617045598687e-12, 6.518210331440214e-09, 8.783602617783837e-13, 6.701444468735928e-09, 1.0768261868323776e-12, 1.0201550479411466e-12, 7.151053703744264e-13, 1.0201550479411466e-12, 1.1737849633727441e-12, 9.018037815333522e-13, 1.1000617045598687e-12]])]\n",
      "cider [(0.0035995160537394665, array([0.        , 0.        , 0.00336537, 0.        , 0.01697264,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.00128755,\n",
      "       0.        , 0.02156863]))]\n",
      "rouge [(0.2767618717501197, array([0.25738397, 0.25738397, 0.36746988, 0.23164557, 0.26349892,\n",
      "       0.21366025, 0.34923664, 0.27853881, 0.34923664, 0.25232678,\n",
      "       0.22222222, 0.27853881]))]\n",
      "Epoch 1/2\n",
      "================================\n",
      "val_running_bleu1: 0.1732 val_running_bleu4: 0.0103 \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to tuple.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8bac73abb3f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'================================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'val_running_bleu{i}: {scores[\"running_bleu\"][i]:.4f} '\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'val_bleu{i + 1}: {scores[\"bleu\"][i]:.4f} '\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-8bac73abb3f1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'================================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'val_running_bleu{i}: {scores[\"running_bleu\"][i]:.4f} '\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'val_bleu{i + 1}: {scores[\"bleu\"][i]:.4f} '\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to tuple.__format__"
     ]
    }
   ],
   "source": [
    "train_loss_min = 100\n",
    "val_loss_min = 100\n",
    "val_bleu4_max = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, val_loss = fit(train_loader, val_loader, model=transformer, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                              desc=f'Epoch {epoch+1} of {NUM_EPOCHS}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores =  evaluate(val_eval_loader, model=transformer, bleu_score_fn=corpus_bleu_score_fn, \n",
    "                           tensor_to_word_fn=tensor2word_fn, desc='Eval Score')\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "        print('================================')\n",
    "        print(''.join([f'val_running_bleu{i}: {scores[\"running_bleu\"][i]:.4f} ' for i in (1, 4)]))\n",
    "        print(''.join([f'val_bleu{i + 1}: {scores[\"bleu\"][i]:.4f} ' for i in range(0, 4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train.transformations = train_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eval_loader = DataLoader(val_eval, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False, collate_fn=eval_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95dc1aff1444a42bea008c16f07eb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='train'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg'] [['a', 'computer', 'screen', 'shows', 'a', 'repair', 'prompt', 'on', 'the', 'screen', '.'], ['a', 'computer', 'screen', 'with', 'a', 'repair', 'automatically', 'pop', 'up'], ['partial', 'computer', 'screen', 'showing', 'the', 'need', 'of', 'repairs'], ['part', 'of', 'a', 'computer', 'monitor', 'showing', 'a', 'computer', 'repair', 'message', '.'], ['the', 'top', 'of', 'a', 'laptop', 'with', 'a', 'blue', 'background', 'and', 'dark', 'blue', 'text', '.'], ['a', 'person', 'is', 'holding', 'a', 'bottle', 'that', 'has', 'medicine', 'for', 'the', 'night', 'time', '.'], ['a', 'bottle', 'of', 'medication', 'has', 'a', 'white', 'twist', 'top', '.'], ['night', 'time', 'medication', 'bottle', 'being', 'held', 'by', 'someone'], ['a', 'person', 'holding', 'a', 'small', 'black', 'bottle', 'of', 'night', 'time'], ['a', 'bottle', 'of', 'what', 'appears', 'to', 'be', 'cough', 'syrup', 'held', 'in', 'hand', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = tqdm(iter(val_eval_loader), desc=f'train')\n",
    "for batch_idx, batch in enumerate(t):\n",
    "    images, captions, lengths, fname = batch\n",
    "    if batch_idx == 0:\n",
    "        print(batch_idx, fname, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
