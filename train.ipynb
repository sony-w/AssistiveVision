{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from loader.dataset import VizwizDataset\n",
    "from loader.images import ImageS3\n",
    "from commons.utils import embedding_matrix, tensor_to_word_fn\n",
    "from models.resnext101 import TransformerAttention\n",
    "from eval.metrics import bleu, cider, rouge, spice, meteor, bleu_score_fn\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %pip install ipywidgets, nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created! imgs = 23431, anns = 100575\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "train = VizwizDataset(dtype='train', ret_type='tensor', copy_img_to_mem=False, device=device, partial=100)\n",
    "vocabulary = train.getVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.10s)\n",
      "creating index...\n",
      "index created! imgs = 7750, anns = 33145\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "val = VizwizDataset(dtype='val', ret_type='tensor', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.41s)\n",
      "creating index...\n",
      "index created! imgs = 23431, anns = 100575\n",
      "tokenizing caption... done!!\n",
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created! imgs = 7750, anns = 33145\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "train_eval = VizwizDataset(dtype='train', ret_type='corpus', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=100)\n",
    "val_eval = VizwizDataset(dtype='val', ret_type='corpus', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created! imgs = 8000, anns = 0\n",
      "no caption found... done!!\n"
     ]
    }
   ],
   "source": [
    "test = VizwizDataset(dtype='test', ret_type='tensor', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'resnext101_attention'\n",
    "GLOVE_DIR = 'annotations/glove'\n",
    "ENCODER_DIM = 2048\n",
    "EMBEDDING_DIM = 300\n",
    "ATTENTION_DIM = 256\n",
    "DECODER_DIM = 256\n",
    "\n",
    "BATCH_SIZE = 10 # 128\n",
    "LOG_INTERVAL = 25 * (256 // BATCH_SIZE)\n",
    "LR = 5e-4\n",
    "NUM_EPOCHS = 2\n",
    "SAVE_FREQ = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014636b52ee641919177b31a5571f522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=378.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(378, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_mtx = embedding_matrix(embedding_dim=EMBEDDING_DIM, word2idx=vocabulary.word2idx, glove_dir=GLOVE_DIR)\n",
    "embedding_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, val_loader, model, loss_fn, optimizer, desc=''):\n",
    "    \n",
    "    # train model\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    t = tqdm(iter(train_loader), desc=f'{desc}_train')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, captions, lengths, fname = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scores, captions_sorted, decode_len, alphas, sort_idx = model(images, captions, lengths)\n",
    "        # exclude <start> and only includes after <start> to <end>\n",
    "        targets = captions_sorted[:, 1:]\n",
    "        # remove pads or timesteps that were not decoded\n",
    "        scores = pack_padded_sequence(scores, decode_len, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, decode_len, batch_first=True)[0]\n",
    "        \n",
    "        loss = loss_fn(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        t.set_postfix({\n",
    "            'loss': train_loss / (batch_idx + 1),\n",
    "            'acc': train_acc / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
    "            print(f'{desc}_train {batch_idx + 1}/{len(train_loader)} '\n",
    "                  f'train_loss: {train_loss / (batch_idx + 1):.4f} '\n",
    "                  f'train_acc: {train_acc / (batch_idx + 1):.4f}')\n",
    "            \n",
    "    train_loss_mean = train_loss / len(train_loader)\n",
    "\n",
    "    # validate model\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    t_val = tqdm(iter(val_loader), desc=f'{desc}_val')\n",
    "    for batch_idx, batch in enumerate(t_val):\n",
    "        images, captions, lengths, fname = batch ## TODO: need to unit test this\n",
    "        \n",
    "        scores, captions_sorted, decode_len, alphas, sort_idx = model(images, captions, lengths)\n",
    "        # exclude <start> and only includes after <start> to <end>\n",
    "        targets = captions_sorted[:, 1:]\n",
    "        # remove pads or timesteps that were not decoded\n",
    "        scores = pack_padded_sequence(scores, decode_len, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, decode_len, batch_first=True)[0]\n",
    "        \n",
    "        loss = loss_fn(scores, targets)\n",
    "        val_acc += (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        t_val.set_postfix({\n",
    "            'loss': val_loss / (batch_idx + 1),\n",
    "            'acc': val_acc / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
    "            print(f'{desc}_val {batch_idx + 1}/{len(val_loader)} '\n",
    "                  f'val_loss: {val_loss / (batch_idx + 1):.4f} '\n",
    "                  f'val_acc: {val_acc / (batch_idx + 1):.4f}')\n",
    "    \n",
    "    val_loss_mean = val_loss / len(val_loader)\n",
    "    \n",
    "    return train_loss_mean, val_loss_mean\n",
    "\n",
    "\n",
    "def evaluate(dataset, model, loss_fn, tensor_to_word_fn, \n",
    "             transformation, dtype='val', device=torch.device('cpu'), desc=''):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    pred_byfname = dict()\n",
    "    caps_byfname = defaultdict(list)\n",
    "    evals = dict()\n",
    "    \n",
    "    bleus, ciders, rouges = [], [], []\n",
    "    running_bleu = [0.0] * 5\n",
    "    \n",
    "    captions_df = dataset.df.groupby('file_name').agg(captions=('caption', list)).reset_index()\n",
    "    \n",
    "    t = tqdm(df.iterrows(), desc=f'{desc}')\n",
    "    for idx, row in enumerate(t):\n",
    "        fname = row[1]['file_name']\n",
    "        captions = row[1]['captions']\n",
    "        \n",
    "        fpath = os.path.join('vizwiz', dtype, fname)\n",
    "        img = transformations(ImageS3.getImage(fpath)).to(device)\n",
    "        \n",
    "        outputs = tensor_to_word_fn(model.sample(img, startseq_idx=vocabulary.word2idx['<start>']).cpu().numpy())\n",
    "        sentence = ' '.join(outputs)\n",
    "        \n",
    "        for i in range(1, 5):\n",
    "            running_bleu[i] += bleu_score_fn(reference_corpus=captions, candidate_corpus=outputs, n=i)\n",
    "        t.set_postfix({\n",
    "            'bleu1': running_bleu[1] / (idx + 1),\n",
    "            'bleu4': running_bleu[4] / (idx + 1),\n",
    "        }, refresh=True)\n",
    "        \n",
    "        pred_byfname[fname] = [sentence]\n",
    "        caps_byfname[fname] = captions\n",
    "    \n",
    "    _bleu = bleu(caps_byfname, pred_byfname)\n",
    "    _cider = cider(caps_byfname, pred_byfname)\n",
    "    _rouge = rouge(caps_byfname, pred_byfname)\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        running_bleu[i] /= len(dataset.df.shape[0])\n",
    "    \n",
    "    bleus.append(_bleu)\n",
    "    ciders.append(_cider)\n",
    "    rouges.append(_rouge)\n",
    "    \n",
    "    evals['bleu'] = bleus\n",
    "    evals['cider'] = ciders\n",
    "    evals['rouge'] = rouges\n",
    "    evals['running_bleu'] = running_bleu\n",
    "    \n",
    "    return evals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary.vocab)\n",
    "transformer = TransformerAttention(encoded_image_size=14, attention_dim=ATTENTION_DIM, embedding_dim=EMBEDDING_DIM, \n",
    "                                   decoder_dim=DECODER_DIM, vocab_size=vocab_size, encoder_dim=ENCODER_DIM, \n",
    "                                   embedding_matrix=embedding_mtx, train_embedding=True).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train.pad_value).to(device)\n",
    "corpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n",
    "tensor2word_fn = tensor_to_word_fn(idx2word=vocabulary.idx2word)\n",
    "\n",
    "params = transformer.parameters()\n",
    "optimizer = torch.optim.Adam(params=params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "eval_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train.transformations = train_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d17e4cdff694546bc8ab82d33a588bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='train'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('VizWiz_train_00000022.jpg', 'VizWiz_train_00000016.jpg', 'VizWiz_train_00000022.jpg', 'VizWiz_train_00000004.jpg', 'VizWiz_train_00000002.jpg', 'VizWiz_train_00000010.jpg', 'VizWiz_train_00000017.jpg', 'VizWiz_train_00000013.jpg', 'VizWiz_train_00000015.jpg', 'VizWiz_train_00000012.jpg')\n",
      "1 ('VizWiz_train_00000021.jpg', 'VizWiz_train_00000011.jpg', 'VizWiz_train_00000006.jpg', 'VizWiz_train_00000003.jpg', 'VizWiz_train_00000006.jpg', 'VizWiz_train_00000014.jpg', 'VizWiz_train_00000008.jpg', 'VizWiz_train_00000014.jpg', 'VizWiz_train_00000007.jpg', 'VizWiz_train_00000013.jpg')\n",
      "2 ('VizWiz_train_00000001.jpg', 'VizWiz_train_00000018.jpg', 'VizWiz_train_00000004.jpg', 'VizWiz_train_00000017.jpg', 'VizWiz_train_00000023.jpg', 'VizWiz_train_00000005.jpg', 'VizWiz_train_00000002.jpg', 'VizWiz_train_00000018.jpg', 'VizWiz_train_00000005.jpg', 'VizWiz_train_00000007.jpg')\n",
      "3 ('VizWiz_train_00000013.jpg', 'VizWiz_train_00000010.jpg', 'VizWiz_train_00000020.jpg', 'VizWiz_train_00000005.jpg', 'VizWiz_train_00000012.jpg', 'VizWiz_train_00000004.jpg', 'VizWiz_train_00000016.jpg', 'VizWiz_train_00000006.jpg', 'VizWiz_train_00000013.jpg', 'VizWiz_train_00000000.jpg')\n",
      "4 ('VizWiz_train_00000008.jpg', 'VizWiz_train_00000010.jpg', 'VizWiz_train_00000003.jpg', 'VizWiz_train_00000015.jpg', 'VizWiz_train_00000003.jpg', 'VizWiz_train_00000022.jpg', 'VizWiz_train_00000013.jpg', 'VizWiz_train_00000001.jpg', 'VizWiz_train_00000009.jpg', 'VizWiz_train_00000018.jpg')\n",
      "5 ('VizWiz_train_00000014.jpg', 'VizWiz_train_00000022.jpg', 'VizWiz_train_00000003.jpg', 'VizWiz_train_00000016.jpg', 'VizWiz_train_00000020.jpg', 'VizWiz_train_00000022.jpg', 'VizWiz_train_00000012.jpg', 'VizWiz_train_00000002.jpg', 'VizWiz_train_00000020.jpg', 'VizWiz_train_00000008.jpg')\n",
      "6 ('VizWiz_train_00000014.jpg', 'VizWiz_train_00000007.jpg', 'VizWiz_train_00000009.jpg', 'VizWiz_train_00000004.jpg', 'VizWiz_train_00000020.jpg', 'VizWiz_train_00000004.jpg', 'VizWiz_train_00000011.jpg', 'VizWiz_train_00000014.jpg', 'VizWiz_train_00000000.jpg', 'VizWiz_train_00000000.jpg')\n",
      "7 ('VizWiz_train_00000002.jpg', 'VizWiz_train_00000007.jpg', 'VizWiz_train_00000005.jpg', 'VizWiz_train_00000012.jpg', 'VizWiz_train_00000012.jpg', 'VizWiz_train_00000011.jpg', 'VizWiz_train_00000006.jpg', 'VizWiz_train_00000003.jpg', 'VizWiz_train_00000017.jpg', 'VizWiz_train_00000002.jpg')\n",
      "8 ('VizWiz_train_00000000.jpg', 'VizWiz_train_00000001.jpg', 'VizWiz_train_00000017.jpg', 'VizWiz_train_00000009.jpg', 'VizWiz_train_00000001.jpg', 'VizWiz_train_00000016.jpg', 'VizWiz_train_00000008.jpg', 'VizWiz_train_00000010.jpg', 'VizWiz_train_00000016.jpg', 'VizWiz_train_00000020.jpg')\n",
      "9 ('VizWiz_train_00000001.jpg', 'VizWiz_train_00000023.jpg', 'VizWiz_train_00000018.jpg', 'VizWiz_train_00000006.jpg', 'VizWiz_train_00000007.jpg', 'VizWiz_train_00000008.jpg', 'VizWiz_train_00000009.jpg', 'VizWiz_train_00000011.jpg', 'VizWiz_train_00000017.jpg', 'VizWiz_train_00000009.jpg')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = tqdm(iter(train_loader), desc=f'train')\n",
    "for batch_idx, batch in enumerate(t):\n",
    "    images, captions, lengths, fname = batch\n",
    "    print(batch_idx, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
