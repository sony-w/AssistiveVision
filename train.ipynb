{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from loader.dataset import VizwizDataset\n",
    "from loader.images import ImageS3\n",
    "from loader.model import ModelS3\n",
    "from commons.utils import embedding_matrix, tensor_to_word_fn\n",
    "from models.resnext101 import TransformerAttention\n",
    "from eval.metrics import bleu, cider, rouge, spice, meteor, bleu_score_fn\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %pip install ipywidgets nltk\n",
    "# sudo yum install java-1.8.0-openjdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created! imgs = 23431, anns = 100575\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "train = VizwizDataset(dtype='train', ret_type='tensor', copy_img_to_mem=False, device=device, partial=100)\n",
    "vocabulary = train.getVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created! imgs = 7750, anns = 33145\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "val = VizwizDataset(dtype='val', ret_type='tensor', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created! imgs = 7750, anns = 33145\n",
      "tokenizing caption... done!!\n"
     ]
    }
   ],
   "source": [
    "val_eval = VizwizDataset(dtype='val', ret_type='corpus', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created! imgs = 8000, anns = 0\n",
      "no caption found... done!!\n"
     ]
    }
   ],
   "source": [
    "test = VizwizDataset(dtype='test', ret_type='corpus', copy_img_to_mem=False, vocabulary=vocabulary, device=device, partial=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'assistive-version'\n",
    "MODEL = 'resnext101_attention'\n",
    "GLOVE_DIR = 'annotations/glove'\n",
    "ENCODER_DIM = 2048\n",
    "EMBEDDING_DIM = 300\n",
    "ATTENTION_DIM = 256\n",
    "DECODER_DIM = 256\n",
    "\n",
    "BATCH_SIZE = 10 # 128\n",
    "LOG_INTERVAL = 25 * (256 // BATCH_SIZE)\n",
    "LR = 5e-4\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "LOCAL_PATH = 'bin/'\n",
    "KEY_PATH = 'bin/'\n",
    "CAPTIONS_PATH = 'captions/'\n",
    "VERSION = 1.0\n",
    "\n",
    "MODEL_NAME = f'{MODEL}_b{BATCH_SIZE}_emb{EMBEDDING_DIM}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421cf363ac0c4d80b231977f0f896518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=378.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(378, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_mtx = embedding_matrix(embedding_dim=EMBEDDING_DIM, word2idx=vocabulary.word2idx, glove_dir=GLOVE_DIR)\n",
    "embedding_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataloaders, model, loss_fn, optimizer, desc=''):\n",
    "    \n",
    "    means = dict()\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        acc = 0.0\n",
    "        loss = 0.0\n",
    "        \n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "            \n",
    "        t = tqdm(iter(dataloaders[phase]), desc=f'{desc} ::: {phase}')\n",
    "        for batch_idx, batch in enumerate(t):\n",
    "            images, captions, lengths, fname, image_id = batch\n",
    "    \n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            scores, captions_sorted, decode_len, alphas, sort_idx = model(images, captions, lengths)\n",
    "            # exclude <start> and only includes after <start> to <end>\n",
    "            targets = captions_sorted[:, 1:]\n",
    "            # remove pads or timesteps that were not decoded\n",
    "            scores = pack_padded_sequence(scores, decode_len, batch_first=True)[0]\n",
    "            targets = pack_padded_sequence(targets, decode_len, batch_first=True)[0]\n",
    "\n",
    "            loss = loss_fn(scores, targets)\n",
    "    \n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            acc += (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n",
    "            loss += loss.item()\n",
    "\n",
    "            t.set_postfix({\n",
    "                'loss': loss / (batch_idx + 1),\n",
    "                'acc': acc / (batch_idx + 1)\n",
    "            }, refresh=True)\n",
    "\n",
    "            if (batch_idx + 1) % LOG_INTERVAL == 0 :\n",
    "                print(f'{desc}_{phase} {batch_idx + 1}/{len(dataloaders[phase])} '\n",
    "                      f'{phase}_loss: {loss / (batch_idx + 1):.4f} '\n",
    "                      f'{phase}_acc: {acc / (batch_idx + 1):.4f}')\n",
    "        \n",
    "        means[phase] = loss / len(dataloaders[phase])\n",
    "    \n",
    "    return means['train'], means['val']\n",
    "    \n",
    "\n",
    "def detokenize(tokens):\n",
    "    return ''.join([' ' + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n",
    "\n",
    "def evaluate(data_loader, model, bleu_score_fn, tensor_to_word_fn, desc=''):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    pred_byfname = dict()\n",
    "    caps_byfname = defaultdict(list)\n",
    "    scores = dict()\n",
    "    \n",
    "    running_bleu = [0.0] * 5\n",
    "    \n",
    "    t = tqdm(iter(data_loader), desc=f'{desc}')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, captions, lengths, fname, image_id = batch\n",
    "        outputs = tensor_to_word_fn(model.sample(images, startseq_idx=vocabulary.word2idx['<start>']).cpu().numpy())\n",
    "        \n",
    "        for i in range(1, 5):\n",
    "            running_bleu[i] += bleu_score_fn(reference_corpus=captions, candidate_corpus=outputs, n=i)\n",
    "        t.set_postfix({\n",
    "            'bleu1': running_bleu[1] / (batch_idx + 1),\n",
    "            'bleu4': running_bleu[4] / (batch_idx + 1)\n",
    "        }, refresh=True)\n",
    "        \n",
    "        for f, o, c in zip(fname, outputs, captions):\n",
    "            if not f in pred_byfname:\n",
    "                pred_byfname[f] = [detokenize(o)]\n",
    "            caps_byfname[f].append(detokenize(c))\n",
    "\n",
    "    # mean running_bleu score\n",
    "    for i in range(1, 5):\n",
    "        running_bleu[i] /= len(data_loader)\n",
    "    scores['avg_bleu'] = running_bleu\n",
    "\n",
    "    # calculate overall score\n",
    "    scores['bleu'] = bleu(caps_byfname, pred_byfname, verbose=0)\n",
    "    scores['cider'] = cider(caps_byfname, pred_byfname)\n",
    "    scores['rouge'] = rouge(caps_byfname, pred_byfname)\n",
    "    #scores['spice'] = spice(caps_byfname, pred_byfname)\n",
    "    #scores['meteor'] = meteor(caps_byfname, pred_byfname)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def generate_captions(dataloader, model, desc=''):\n",
    "    rlist = []\n",
    "    \n",
    "    t = tqdm(iter(dataloader), desc=f'{desc}')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, fname, image_id = batch\n",
    "        outputs = tensor2word_fn(model.sample(images, startseq_idx=vocabulary.word2idx['<start>']).cpu().numpy())\n",
    "\n",
    "        for out, img in zip(outputs, image_id):\n",
    "            result = dict(\n",
    "                image_id = int(img),\n",
    "                caption = detokenize(out)\n",
    "            )\n",
    "            rlist.append(result)\n",
    "\n",
    "    results = dict(\n",
    "        results = rlist\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary.vocab)\n",
    "transformer = TransformerAttention(encoded_image_size=14, attention_dim=ATTENTION_DIM, embedding_dim=EMBEDDING_DIM, \n",
    "                                   decoder_dim=DECODER_DIM, vocab_size=vocab_size, encoder_dim=ENCODER_DIM, \n",
    "                                   embedding_matrix=embedding_mtx, train_embedding=True).to(device) \n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train.pad_value).to(device)\n",
    "corpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n",
    "tensor2word_fn = tensor_to_word_fn(idx2word=vocabulary.idx2word)\n",
    "\n",
    "params = transformer.parameters()\n",
    "optimizer = torch.optim.Adam(params=params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "eval_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train.transformations = train_transformations\n",
    "val.transformations = train_transformations\n",
    "val_eval.transformations = eval_transformations\n",
    "test.transformations = eval_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict(\n",
    "    train = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False),\n",
    "    val = DataLoader(val, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    ")\n",
    "#train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    "#val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    "\n",
    "eval_collate_fn = lambda batch: (torch.stack([x[0] for x in batch]), [x[1] for x in batch], [x[2] for x in batch], \n",
    "                                 [x[3] for x in batch], [x[4] for x in batch])\n",
    "test_collate_fn = lambda batch: (torch.stack([x[0] for x in batch]), [x[1] for x in batch], [x[2] for x in batch])\n",
    "\n",
    "val_eval_loader = DataLoader(val_eval, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False, collate_fn=eval_collate_fn)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False, collate_fn=test_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-25 17:53:35.181 pytorch-1-6-cpu-py36--ml-t3-medium-370ee60fbc7a856e8f67ac271515:1024 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-06-25 17:53:35.346 pytorch-1-6-cpu-py36--ml-t3-medium-370ee60fbc7a856e8f67ac271515:1024 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e06a06e281d41d6bb7e2260246587e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1 of 2 ::: train'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fd522143ea41d382828f48931fff36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1 of 2 ::: val'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500dafb52a3f4f2997a2921e7353b79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Eval Score'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "===============================================================================================\n",
      "val_avg_bleu1: 0.1861 val_avg_bleu2: 0.0367 val_avg_bleu3: 0.0181 val_avg_bleu4: 0.0114 \n",
      "val_bleu1    : 0.2948 val_bleu2    : 0.0000 val_bleu3    : 0.0000 val_bleu4    : 0.0000 \n",
      "val_cider    : 0.0000\n",
      "val_rouge    : 0.2798\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5f1b4511e34ed6adec01dfb34d605b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2 of 2 ::: train'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2962426b4c844a1d8ce11ff1af77a317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2 of 2 ::: val'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836538209e7b4d7681662345fdb88be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Eval Score'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/2\n",
      "===============================================================================================\n",
      "val_avg_bleu1: 0.1913 val_avg_bleu2: 0.0378 val_avg_bleu3: 0.0187 val_avg_bleu4: 0.0118 \n",
      "val_bleu1    : 0.3018 val_bleu2    : 0.0570 val_bleu3    : 0.0000 val_bleu4    : 0.0000 \n",
      "val_cider    : 0.0044\n",
      "val_rouge    : 0.2892\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loss_min = 100\n",
    "val_loss_min = 100\n",
    "val_avg_bleu4_max = 0.0\n",
    "\n",
    "model_bin = ModelS3()\n",
    "transformer_best = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, val_loss = fit(dataloaders, model=transformer, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                              desc=f'Epoch {epoch+1} of {NUM_EPOCHS}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores =  evaluate(val_eval_loader, model=transformer, bleu_score_fn=corpus_bleu_score_fn, \n",
    "                           tensor_to_word_fn=tensor2word_fn, desc='Eval Score')\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "        print('=' * 95)\n",
    "        print(''.join([f'val_avg_bleu{i}: {scores[\"avg_bleu\"][i]:.4f} ' for i in range(1, 5)]))\n",
    "        print(''.join([f'val_bleu{i + 1}{\":\":>5} {scores[\"bleu\"][0][i]:.4f} ' for i in range(0, 4)]))\n",
    "        print(f'val_cider{\":\":>5} {scores[\"cider\"][0]:.4f}')\n",
    "        print(f'val_rouge{\":\":>5} {scores[\"rouge\"][0]:.4f}')\n",
    "        #print(f'val_spcie{\":\":>5} {scores[\"spice\"][0]:.4f}')\n",
    "        #print(f'val_meteor{\":\":>5} {scores[\"meteor\"][0]:.4f}')\n",
    "        print('-' * 95)\n",
    "        \n",
    "        state = dict(\n",
    "            epoch = epoch + 1,\n",
    "            state_dict = transformer.state_dict(),\n",
    "            train_loss_latest = train_loss,\n",
    "            val_loss_latest = val_loss,\n",
    "            train_loss_min = min(train_loss, train_loss_min),\n",
    "            val_loss_min = min(val_loss, val_loss_min),\n",
    "            val_avg_bleu1 = scores['avg_bleu'][1],\n",
    "            val_avg_bleu4 = scores['avg_bleu'][4],\n",
    "            val_avg_bleu4_max = max(scores['avg_bleu'][4], val_avg_bleu4_max),\n",
    "            val_bleu1 = scores['bleu'][0][0],\n",
    "            val_bleu4 = scores['bleu'][0][3],\n",
    "            val_cider = scores['cider'][0],\n",
    "            val_rouge = scores['rouge'][0]\n",
    "        )\n",
    "        \n",
    "        if scores['avg_bleu'][4] > val_avg_bleu4_max:\n",
    "            val_avg_bleu4_max = scores['avg_bleu'][4]\n",
    "            fname = f'{MODEL_NAME}_best_v{VERSION}.pt'\n",
    "            # keep the best transformer\n",
    "            transformer_best = transformer\n",
    "            model_bin.save(state, os.path.join(LOCAL_PATH, fname), os.path.join(KEY_PATH, fname))\n",
    "\n",
    "fname = f'{MODEL_NAME}_ep{NUM_EPOCHS}_latest_v{VERSION}.pt'\n",
    "model_bin.save(state, os.path.join(LOCAL_PATH, fname), os.path.join(KEY_PATH, fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a47a25fb2e4e9e8ccd2a02838bfa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='captioning ::: test'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': [{'image_id': 31181, 'caption': 'a a a a a a'},\n",
       "  {'image_id': 31182, 'caption': 'a a a a a'},\n",
       "  {'image_id': 31183, 'caption': 'a a of a a'},\n",
       "  {'image_id': 31184, 'caption': 'a a a a a'},\n",
       "  {'image_id': 31185, 'caption': 'a a of of a a a a'},\n",
       "  {'image_id': 31186, 'caption': 'a a a a a a'},\n",
       "  {'image_id': 31187, 'caption': 'a a a a a a a'},\n",
       "  {'image_id': 31188, 'caption': 'a a of a a'},\n",
       "  {'image_id': 31189, 'caption': 'a a a a a a a'},\n",
       "  {'image_id': 31190, 'caption': 'a a a a a a'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = generate_captions(test_loader, model=transformer, desc='captioning ::: test')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save captions to s3\n",
    "fname = f'{MODEL_NAME}_ep{NUM_EPOCHS}_latest_v{VERSION}.json'\n",
    "model_bin.save_captions(results, os.path.join(CAPTIONS_PATH, fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'image_id': 31181, 'caption': 'a a a a a a'},\n",
       "  {'image_id': 31182, 'caption': 'a a a a a'},\n",
       "  {'image_id': 31183, 'caption': 'a a of a a'},\n",
       "  {'image_id': 31184, 'caption': 'a a a a a'},\n",
       "  {'image_id': 31185, 'caption': 'a a of of a a a a'},\n",
       "  {'image_id': 31186, 'caption': 'a a a a a a'},\n",
       "  {'image_id': 31187, 'caption': 'a a a a a a a'},\n",
       "  {'image_id': 31188, 'caption': 'a a of a a'},\n",
       "  {'image_id': 31189, 'caption': 'a a a a a a a'},\n",
       "  {'image_id': 31190, 'caption': 'a a a a a a'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load captions from s3\n",
    "results = model_bin.load_captions(os.path.join(CAPTIONS_PATH, fname))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a a of a a'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# prepare for inference\n",
    "imageS3 = ImageS3()\n",
    "fpath = os.path.join('vizwiz', 'test', 'VizWiz_test_00000002.jpg')\n",
    "img = eval_transformations(imageS3.getImage(fpath)).to(device)\n",
    "#caption = [vocabulary.idx2word[idx.item()] for idx in transformer.sample(img.unsqueeze(0), vocabulary.word2idx['<start>'])[0] \n",
    "#                 if idx.item() != vocabulary.word2idx['<end>']]\n",
    "caption = tensor2word_fn(transformer.sample(img.unsqueeze(0), startseq_idx=vocabulary.word2idx['<start>']).cpu().numpy())[0]\n",
    "detokenize(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for experiment only\n",
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train.transformations = train_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eval_loader = DataLoader(val_eval, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False, collate_fn=eval_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95dc1aff1444a42bea008c16f07eb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='train'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000000.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg', 'VizWiz_val_00000001.jpg'] [['a', 'computer', 'screen', 'shows', 'a', 'repair', 'prompt', 'on', 'the', 'screen', '.'], ['a', 'computer', 'screen', 'with', 'a', 'repair', 'automatically', 'pop', 'up'], ['partial', 'computer', 'screen', 'showing', 'the', 'need', 'of', 'repairs'], ['part', 'of', 'a', 'computer', 'monitor', 'showing', 'a', 'computer', 'repair', 'message', '.'], ['the', 'top', 'of', 'a', 'laptop', 'with', 'a', 'blue', 'background', 'and', 'dark', 'blue', 'text', '.'], ['a', 'person', 'is', 'holding', 'a', 'bottle', 'that', 'has', 'medicine', 'for', 'the', 'night', 'time', '.'], ['a', 'bottle', 'of', 'medication', 'has', 'a', 'white', 'twist', 'top', '.'], ['night', 'time', 'medication', 'bottle', 'being', 'held', 'by', 'someone'], ['a', 'person', 'holding', 'a', 'small', 'black', 'bottle', 'of', 'night', 'time'], ['a', 'bottle', 'of', 'what', 'appears', 'to', 'be', 'cough', 'syrup', 'held', 'in', 'hand', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = tqdm(iter(val_eval_loader), desc=f'train')\n",
    "for batch_idx, batch in enumerate(t):\n",
    "    images, captions, lengths, fname = batch\n",
    "    if batch_idx == 0:\n",
    "        print(batch_idx, fname, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
